{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Coding Concepts for Regression Analysis\n",
    "\n",
    "## Theoretical Calculations\n",
    "\n",
    "### Sum of Squares\n",
    "In Regular Form \n",
    "- $SSto = \\sum_{i=1}^n(Y_i-\\bar{Y})^2$\n",
    "    - $\\frac{SSto}{\\sigma^2} \\sim \\chi_{n-1}^2$\n",
    "- $SSE = \\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2$\n",
    "    - $\\frac{SSE}{\\sigma^2} \\sim \\chi_{p}^2$\n",
    "- $SSR = \\sum_{i=1}^n(\\hat{Y}_i-\\bar{Y})^2$\n",
    "    - $\\frac{SSR}{\\sigma^2} \\sim \\chi_{n-p-1}^2$\n",
    "    \n",
    "In Matrix Form\n",
    "- $SSto = \\underline{Y}^T\\underline{Y} - n\\bar{Y}^2 = \\underline{Y}^T\\underline{Y} - \\frac{1}{n}\\underline{J}\\underline{Y}$\n",
    "- $SSE = \\underline{Y}^T\\underline{Y} - \\textbf{H}\\underline{Y}$\n",
    "- $SSR = \\underline{Y}^T\\textbf{H}\\underline{Y} - n\\hat{Y}^2 = \\underline{Y}^T\\textbf{H}\\underline{Y} - \\frac{1}{n}\\underline{J}\\underline{Y}$\n",
    "- $\\underline{J}$ is an $n\\times n$ matrix of 1s\n",
    "    \n",
    "### Matrix Form of Multible Linear Regression\n",
    "- $\\underline{Y} = \\textbf{X}\\underline{\\beta} + \\underline{\\epsilon}$\n",
    "    - $\\underline{Y}=n\\times 1,\\;\\textbf{X}=n\\times q,\\;\\underline{\\beta}=q\\times1,\\;\\underline{\\epsilon}=n\\times1,\\;q=p+1$\n",
    "    - $\\underline{Y} \\sim N_n(\\textbf{X}\\underline{\\beta},\\;\\sigma^2\\textbf{I}_n)$\n",
    "    - $\\underline{\\epsilon} \\sim N(\\textbf{0},\\;\\sigma^2\\textbf{I}_n)$\n",
    "    - $\\underline{\\hat{\\beta}} \\sim N(\\underline{\\beta},\\;\\sigma^2(\\textbf{X}^T\\textbf{X})^{-1})$\n",
    "- $\\begin{equation}\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2\\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "\\vdots & \\vdots  & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\dots & x_{np} \\\\\n",
    "\\end{bmatrix} \\times \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2\\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}\\end{equation}$\n",
    "\n",
    "#### Statistics Matrix Properties\n",
    "- $E(\\underline{a} + \\textbf{A}\\underline{Y}) = \\underline{a} + \\textbf{A}E(\\underline{Y})$\n",
    "- $var(\\underline{a} + \\textbf{A}\\underline{Y}) = var(\\textbf{A}\\underline{Y}) = \\textbf{A}var(\\underline{Y})\\textbf{A}^T$ \n",
    "\n",
    "#### Matrix Least Squares Estimates\n",
    "- $\\underline{\\hat{\\beta}}$ minimizes $Q = \\sum_{i=1}^n(Y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij})^2 = (\\underline{Y}-\\textbf{X}\\underline{\\beta})^T(\\underline{Y}-\\textbf{X}\\underline{\\beta})$\n",
    "    - solution is $\\textbf{X}^T\\textbf{X}\\underline{\\hat{\\beta}} = \\textbf{X}^T\\underline{Y}$\n",
    "    - if $\\textbf{X}^T\\textbf{X}$ is invertible, estimates are $\\underline{\\hat{\\beta}} = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\underline{Y}$\n",
    "    \n",
    "#### Hat Matrix\n",
    "- $\\textbf{H} = \\textbf{X}\\underline{\\hat{\\beta}} = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T$\n",
    "    - $\\textbf{H}$ is $n\\times n$\n",
    "    - symmetric: $\\textbf{H}^T=\\textbf{H}$\n",
    "    - idempotent: $\\textbf{H}^2 = \\textbf{H}$\n",
    "    - trace: $tr(\\textbf{H}) = q,\\;tr(\\textbf{I}-\\textbf{H}) = n-q$\n",
    "        - $\\textbf{I}_n-\\textbf{H}$ is symmetric and idempotent\n",
    "- residuals: $\\underline{e} = \\underline{Y} - \\textbf{X}\\underline{\\hat{\\beta}} = (\\textbf{I}-\\textbf{H})\\underline{Y}$\n",
    "    - $\\underline{e} \\sim N_n(\\textbf{0},\\;\\sigma^2(\\textbf{I}-\\textbf{H}))$\n",
    "- fitted values: $\\underline{\\hat{Y}} = \\textbf{X}\\underline{\\hat{\\beta}} = \\textbf{H}\\underline{Y}$\n",
    "    - $\\underline{\\hat{Y}} \\sim N_n(\\textbf{0},\\;\\sigma^2\\textbf{H})$\n",
    "    \n",
    "### Matrix Form for Polynomial Regression\n",
    "- $\\begin{equation}\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2\\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 & \\dots & x_1^h \\\\\n",
    "1 & x_2 & x_2^2 & \\dots & x_2^h \\\\\n",
    "\\vdots & \\vdots  & \\ddots & \\vdots\\\\\n",
    "1 & x_n & x_n^2 & \\dots & x_n^h \\\\\n",
    "\\end{bmatrix} \\times \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots \\\\\n",
    "\\beta_h\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2\\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}\\end{equation}$\n",
    "\n",
    "### Non-Constant Variance\n",
    "- $\\underline{Y} = \\textbf{X} + \\underline{\\beta} + \\underline{\\epsilon}$ where $\\underline{\\epsilon} \\sim N(\\textbf{0},\\sigma^2\\textbf{W}^{-1}$\n",
    "    - $\\textbf{W} = diag(w_1,...,w_n)$\n",
    "    - $\\sigma^2\\textbf{W}^{-1} = \\begin{bmatrix}\n",
    "    \\frac{\\sigma^2}{w_1} & 0 & \\dots & 0 \\\\\n",
    "    0 & \\frac{\\sigma^2}{w_2} & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & \\frac{\\sigma^2}{w_n} \\\\ \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    \\sigma_1^2 & 0 & \\dots & 0 \\\\\n",
    "    0 & \\sigma_2^2 & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & \\sigma_n^2 \\\\\n",
    "    \\end{bmatrix}$\n",
    "    \n",
    "Weighted Least Squares Estimates\n",
    "- $\\underline{\\hat{\\beta}}_{WLS}$ is an unbiased estimator that minimizes $Q_w(\\beta) = \\sum_{i=1}^nw_i\\epsilon_i^2$\n",
    "- $\\underline{\\hat{\\beta}}_{WLS} = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{W}\\underline{Y}$\n",
    "- $var(\\underline{\\hat{\\beta}}_{WLS}) = \\sigma^2(\\textbf{X}^T\\textbf{W}\\textbf{X})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "### Examine Full Linear Model\n",
    "#### Create Model\n",
    "$E(Y_i)=\\beta_0+\\beta_1x_{i1}+...+\\beta_px_{ip}$\n",
    "\n",
    "```\n",
    "# create full linear model\n",
    "fullLM <- lm(yVar ~ ., data=data)\n",
    "# summary of the full linear model\n",
    "summary(fullLM)\n",
    "```\n",
    "\n",
    "#### Plots About Predictor Correlation\n",
    "Scatterplot Matrix\n",
    "- look at linear and nonlinear relationships between variables\n",
    "- look at whether relationships are positive or negative <br>\n",
    "\n",
    "Added Variable Plot\n",
    "- slope close to 0: predictor contains no additional information for predicting the response\n",
    "- slope is linear: predictor contains useful information for predicting the response\n",
    "- slope is nonlinear: predictor contains useful information for predicting the response, but some assumptions are not met\n",
    "\n",
    "```\n",
    "# scatterplot matrix\n",
    "    # method 1\n",
    "pairs(~ yVar + xVar1 + xVar2 + xVar3)\n",
    "    # method 2\n",
    "scatterplotMatrix(~ yVar + xVar1 + xVar2 + xVar3)\n",
    "# added variable plot\n",
    "avPlots(fullLM)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Assumptions\n",
    "Assumptions\n",
    "- the mean response E($Y_i$) is a linear function of $x$\n",
    "- errors are independent\n",
    "- errors are normally distributed\n",
    "- errors have a constant variance\n",
    "\n",
    "#### Fitted Values and Residuals\n",
    "- $\\hat{Y}_i = \\hat{E}(Y_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{i1} + ... + \\hat{\\beta}_px_{ip}$ \n",
    "- $e_i = Y_i - \\hat{Y}_i$\n",
    "    - $\\sum_{i=1}^ne_i=0$\n",
    "\n",
    "```\n",
    "# fitted values\n",
    "fittedValues <- fit(dollar)fitted.values\n",
    "# residuals\n",
    "resid <- fit(dollar)residuals\n",
    "```\n",
    "\n",
    "#### Scatterplot of Residuals vs Fitted Values\n",
    "- linearity: residuals scattered around e=0\n",
    "- constant variance: residuals form a horizontal band around e=0\n",
    "- no outliers: no residuals stand out from the basic pattern\n",
    "\n",
    "```\n",
    "# scatterplot of residuals vs fitted\n",
    "    # method 1\n",
    "plot(fittedValues, resid, xlab='Fitted Values', ylab='Residuals', main='Residuals vs Fitted Values')\n",
    "abline(h=0, col='red')\n",
    "    # method 2\n",
    "plot(fit, which = 1)\n",
    "```\n",
    "\n",
    "#### Normal Probability Plot\n",
    "- normality: Q-Q plot is linear\n",
    "\n",
    "```\n",
    "# normal probability plot\n",
    "    # method 1\n",
    "qqnorm(resid)\n",
    "qqline(resid)\n",
    "    # method 2\n",
    "plot(fit, which = 2)\n",
    "# nonconstant variance test, constant variance if p value greater than 0.05\n",
    "ncvTest(fullLM)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data\n",
    "- $Y = a + bx^\\alpha$, let $\\tilde{x} = x^\\alpha$ and $Y = a + b\\tilde{x}$\n",
    "- $Y^\\beta = a + bx$, let $\\tilde{Y} = Y^\\beta$ and $\\tilde{Y} = a + bx$\n",
    "\n",
    "#### Invariance Transformation (for single predictor)\n",
    "- $\\begin{equation}x_i^{(\\lambda)}=\\begin{cases}\\frac{x_i^\\lambda-1}{\\lambda};\\;\\lambda \\neq 0 \\\\ln(x_i);\\;\\lambda=0 \\end{cases}\\end{equation}$\n",
    "- $Y_i = \\beta_0 + \\beta_1x_{i1}^{(\\lambda)} + ... + \\beta_px_{ip}^{(\\lambda)} + \\epsilon_i$\n",
    "\n",
    "```\n",
    "# invariance transformation\n",
    "library(car)\n",
    "invTranPlot(yVar ~ xVar, lambda = c(-1,0,1), optimal = F)\n",
    "```\n",
    "\n",
    "#### Power Transform (for multiple predictors)\n",
    "\n",
    "```\n",
    "# power transformation\n",
    "library(car)\n",
    "predTr <- powerTransform(cbind(xVar1, xVar2, xVar3) ~ 1, data=data)\n",
    "summary(predTr)\n",
    "```\n",
    "\n",
    "#### Box-Cox Transformation (for response)\n",
    "- $\\begin{equation}Y_i^{(\\lambda)}=\\begin{cases}\\frac{Y_i^\\lambda-1}{\\lambda};\\;\\lambda \\neq 0 \\\\ln(Y_i);\\;\\lambda=0 \\end{cases}\\end{equation}$\n",
    "- $Y_i^{(\\lambda)} = \\beta_0 + \\beta_1x_{i1} + ... + \\beta_px_{ip} + \\epsilon_i$\n",
    "- choose a round number for $\\hat{\\lambda}$ like $-1, 0, 1, 0.5, ...$ since round numbers are easier to transform back to original units\n",
    "    - $\\hat{\\lambda} = 1$ means no transformation is needed to stabilize the variance\n",
    "    - $\\hat{\\lambda} = 0$ means a log transformation is needed to stabilize the variance\n",
    "- try several transformations suggested by the confidence interval\n",
    "\n",
    "```\n",
    "# box cox transformation\n",
    "    # method 1\n",
    "library(MASS)\n",
    "bc <- boxcox(lm(fit), lambda=seq(-1,1,by=.1))\n",
    "bestLambda <- bc(dollar)x[which(bc(dollar)y==max(bc(dollar)y))]\n",
    "    # method 2\n",
    "library(car)\n",
    "bc <- boxCox(lm(fit), lambda=seq(-1,1,by=.1))\n",
    "bestLambda <- bc(dollar)x[which.max(bc(dollar)y)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Problematic Datapoints\n",
    "- Outlier: data point whose response does not follow the trend of the rest of the data.\n",
    "- High Leverage: data point with extreme predictor values.\n",
    "    - For multiple predictors, can be extreme for one or more predictors, or unusual combinations of predictors.\n",
    "- Influential Point: if data point has too much of an influence some part of regression analysis.\n",
    "\n",
    "#### Outliers\n",
    "- $t_i$ are studentized residuals\n",
    "    - $t_i = \\frac{e_{ii}}{\\hat{\\sigma}(i)\\sqrt{1-h_{ii}}} = r_i(\\frac{n-p-2}{n-p-1-r_i^2})^{1/2}$\n",
    "        - $r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1+h_{ii}}}$ are standardized residuals\n",
    "    - omits the $i^{th}$ element of Y and the $i^{th}$ row of x\n",
    "- if $|t_i| > 2$, $Y_i$ is an outlier\n",
    "\n",
    "```\n",
    "# find the row with the maximum magnitude\n",
    "sResid <- rstudent(fit)\n",
    "pos <- abs(sResid)\n",
    "which(pos == max(pos))\n",
    "# test for outliers, gives largest outlier and its studentized residual\n",
    "outlierTest(fit)\n",
    "```\n",
    "\n",
    "#### Leverage Points\n",
    "- if Cook's distance $D_i>\\frac{4}{n-p-1}$, $Y_i$ is a leverage point\n",
    "    - $D_i = \\frac{(\\underline{\\hat{Y}}-\\underline{\\hat{Y}}(i))^T(\\underline{\\hat{Y}}-\\underline{\\hat{Y}}(i))}{\\hat{\\sigma}^2(p+1)} = frac{r_i^2\\times h_{ii}}{(p+1)(1-h_{ii})}$\n",
    "\n",
    "```\n",
    "n <- length(data(dollar)yVar)\n",
    "# plot Cook's distance against rows to see if there is a leverage point\n",
    "plot(fit, which=4)\n",
    "# get values for cooks distance\n",
    "cooks <- cooks.distance(fit)\n",
    "which(cooks > 4/(n-p-1)\n",
    "```\n",
    "\n",
    "#### Influential Points\n",
    "- if hat value $h_{ii}>\\frac{3(p+1)}{n}$ or $h_{ii}>\\frac{2(p+1)}{n}$, $Y_i$ is an influential point\n",
    "    - $h_{ii} \\in [0,1]$ and $\\sum_{i=1}^nh_{ii}=p+1$\n",
    "    - leverage in simple linear regression is $h_{ii} = \\frac{1}{nSxx}\\sum_{j=1}^n(x_j-x_i)^2$\n",
    "\n",
    "```\n",
    "n <- length(data(dollar)yVar)\n",
    "# find hat values\n",
    "hats <- hatvalues(fit)\n",
    "which(hats > 2*sum(hats)/n)\n",
    "which(hats > 3*sum(hats)/n\n",
    "```\n",
    "\n",
    "#### Impact of Points\n",
    "```\n",
    "# plot for hat values and cook's distance\n",
    "influenceIndexPlot(fit, vars=c('hat', 'Cook'), id=TRUE)\n",
    "# plot beta estimates obtained from dropping data points\n",
    "betaHatNoti <- influence(fit)(dollar)coefficients\n",
    "panelFun <- function(x, y, ...){\n",
    "    points(x, y, ...)\n",
    "    dataEllipse(x, y, plot.points=FALSE, levels=c(.90))\n",
    "    showLabels(x, y, labels=rownames(data), method='mahal', n=numberOfPointsExamined)\n",
    "}\n",
    "# pairwise scatterplots with all the points labeled\n",
    "pairs(betaHatNoti, panel=panelFun)\n",
    "```\n",
    "\n",
    "#### Remove Point\n",
    "```\n",
    "# remove outlier\n",
    "dataWithoutOutlier <- data[-rowWithOutlier,]\n",
    "# new linear model without outlier\n",
    "lmWithoutOutlier <- lm(yVar ~ ., data=dataWithoutOutlier)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor Selection\n",
    "\n",
    "#### Stepwise Regression\n",
    "- add or take away predictors until lowest AIC or BIC achieved\n",
    "- forward and backward selection produce different models <br>\n",
    "Information Criterion <br>\n",
    "- evaluates models based on goodness of fit and complexity\n",
    "- lower values indicate a better model\n",
    "\n",
    "Akaike's Information Criterion\n",
    "- $AIC = nln(\\frac{SSE}{n}) + 2(p+1)$ <br>\n",
    "\n",
    "Bayesian Information Criterion\n",
    "- $BIC = nln(\\frac{SSE}{n}) + ln(n)(p+1)$\n",
    "- places higher penalty on number of predictors than AIC\n",
    "\n",
    "```\n",
    "# parameters\n",
    "modEmpty <- lm(yVar ~ 1, data=data)\n",
    "modFull <- lm(yVar ~ ., data=data)\n",
    "n <- length(data(dollar)yVar)\n",
    "# forward selection with AIC\n",
    "forwardAIC <- step(modEmpty, scope=list(lower=modEmpty, upper=modFull), direction='forward')\n",
    "# forward selection with BIC\n",
    "forwardBIC <- step(modEmpty, scope=list(lower=modEmpty, upper=modFull), direction='forward', k=log(n), trace=0)\n",
    "# backward selection with AIC\n",
    "backwardAIC <- step(modFull, scope=list(lower=modEmpty, upper=modFull), direction='backward')\n",
    "# backward selection with BIC\n",
    "backwardBIC <- step(modFull, scope=list(lower=modEmpty, upper=modFull), direction='backward', k=log(n), trace=0)\n",
    "```\n",
    "\n",
    "#### Regression Subsets\n",
    "R Squared\n",
    "- $R^2=\\frac{SSR}{SSTO} = n$\n",
    "- $100 \\times x$% of the variability of Y is explained by its linear relationship with predictors\n",
    "- choose model with the largest increase in R squared <br>\n",
    "\n",
    "Adjusted R Squared\n",
    "- $adjusted\\;R^2=1-\\frac{n-1}{n-p-1}(1-R^2) $\n",
    "- choose model with the largest adjusted R squared <br>\n",
    "\n",
    "Mallows Cp Statistic\n",
    "- $Cp = \\frac{SSE_p}{MSE_{full}} + 2q - n$\n",
    "- choose simplist model with smallest Cp near q=p+1\n",
    "\n",
    "```\n",
    "library(leaps)\n",
    "# get regression subsets object\n",
    "modReg <- regsubsets(cbind(xVar1, xVar2, xVar3), yVar, data=data)\n",
    "sumReg <- summary(modReg)\n",
    "# output best model for 1 to 3 predictors\n",
    "sumReg(dollar)which\n",
    "# output R squared\n",
    "sumReg(dollar)rsq\n",
    "# output adjusted R squared\n",
    "sumReg(dollar)adjr2\n",
    "# output Mallow's Cp statistic\n",
    "sumReg(dollar)cp\n",
    "# output BIC\n",
    "sumReg(dollar)bic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Tests for Multiple Linear Regression\n",
    "#### Global F Test\n",
    "Are there significant parameters in the model?\n",
    "- $H_0:\\beta_1=...=\\beta_p=0$ vs $H_a:$ at least one $\\beta_j \\neq 0$ for $j=1,...,p$\n",
    "    - all parameters are insignificant if p value > 0.05\n",
    "- test statistic: $F = \\frac{MSR}{MSE}$ and under the null hypothesis $F\\sim F_{p,\\;n-p-1}$\n",
    "\n",
    "```\n",
    "# global F test\n",
    "    # method 1\n",
    "# look at the F-statistic to see global F test statistic and p value\n",
    "sumGlobal <- summary(fullLM)\n",
    "    # method 2\n",
    "emptyLM <- lm(yVar ~ 1)\n",
    "anovaGlobal <- anova(emptyLM, fullLM)\n",
    "anovaGlobal(dollar)'Pr(>F)'[2]\n",
    "```\n",
    "\n",
    "#### Partial F Test for One Parameter\n",
    "- $(t_n)^2 = F_{1,n}$ so the t and F tests are equivalent\n",
    "\n",
    "Is the third parameter significant?\n",
    "- since it is one parameter, F and t tests yield the same result\n",
    "- $H_0: \\beta_3=0$ vs $H_a: \\beta_3\\neq 0$\n",
    "    - $\\beta_3$ is insignificant if p value > 0.05\n",
    "- test statistic: $T_3=\\frac{\\hat\\beta_3}{SE(\\hat\\beta_3)}$ and under the null hypothesis $T_3\\sim t_{n-p-1}$\n",
    "    - $SE(\\hat\\beta_k = \\sqrt{MSE[(\\textbf{X}^T\\textbf{X})^{-1}]_{kk}}$\n",
    "    \n",
    "```\n",
    "# partial F test for one parameter\n",
    "    # method 1\n",
    "# look at the Pr(>|t|) column to see the partial F test corresponding to each Bk\n",
    "sum <- summary(fullLM)\n",
    "    # method 2\n",
    "redB3LM <- lm(yVar ~ xVar1 + xVar2)\n",
    "anovaTestB3 <- anvoa(redB3LM, fullLM)\n",
    "anovaTestB3(dollar)'Pr(>F)'[2]\n",
    "```\n",
    "\n",
    "#### Partial F Test for Multiple Parameters\n",
    "Are the second and/or third parameters significant? \n",
    "- $H_0: \\beta_2=\\beta_3=0$ vs $H_a:$ at least one $\\beta_j\\neq 0$ for $j=2,3$\n",
    "    - $\\beta_2$ and/or $\\beta_3$ is insignificant if p value > 0.05\n",
    "- test statistic: $F=\\frac{\\frac{SSE_{red}-SSE{full}}{df_{red}-df_{full}}}{\\frac{SSE_{full}}{df_{full}}}$ and under the null hypothesis $F \\sim F_{number\\;of\\;predictors,\\;n-p-1}$\n",
    "\n",
    "```\n",
    "# partial F test for multiple parameters\n",
    "redB2B3LM <- lm(yVar ~ xVar1)\n",
    "anovaTestB2B3 <- anova(redB2B3LM, fullLM)\n",
    "anovaTestB2B3(dollar)'Pr(>F)'[2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model \n",
    "\n",
    "### ANOVA Table for Multiple Linear Regression\n",
    "|Sources of Variation|Sum of Squares|Degrees of Freedom|Mean Square Value|F Observed|\n",
    "|----|----|----|----|----|\n",
    "|Regression|$SSR$|$p$|$MSR = \\frac{SSR}{p}$|$F=\\frac{MSR}{MSE}$|\n",
    "|Error|$SSE$|$n-p-1$|$MSE = \\frac{SSE}{n-p-1}$| - |\n",
    "|Total|$SSto$|$n-1$| - | - |\n",
    "\n",
    "### Sequential Sum fo Squares\n",
    "|Sources of Variation|Sum of Squares Value|Degrees of Freedom|Mean Square Value|F Observed Value|Null Hypothesis|\n",
    "|----|----|----|----|----|----|\n",
    "|$x_1$|$SSR(x_1)$|$1$|$MSR(x_1)$|$F=\\frac{MSR(x_1)}{MSE}$|$\\beta_1$ = 0|\n",
    "|$x_2|x_1$|$SSR(x_2|x_1)$|$1$|$MSR(x_2|x_1)$|$F=\\frac{MSR(x_2|x_1)}{MSE}$|$\\beta_2$ = 0|\n",
    "|$x_3|x_2,\\;x_1$|$SSR(x_3|x_2,x_1)$|$1$|$MSR(x_3|x_2,x_1)$|$F=\\frac{MSR(x_3|x_2,x_1)}{MSE}$|$\\beta_3$ = 0|\n",
    "|Error|$SSE(x_1,x_2,x_3)$|$n-p-1$|$MSE$| - | - |\n",
    "|Total|$SSto$|$n-1$| - | - | - |\n",
    "\n",
    "- the increase in regression sum of squares when one or more predictors are added to the model\n",
    "    - $SSR(x_2|x_1)$: increase in sum of squares from adding $x_2$ to a model containing $x_1$\n",
    "        - $SSR(x_2|x_1) = SSR(x_2,x_1) - SSR(x_1)$\n",
    "    - $SSR(x_3,x_2|x_1)$: increase in sum of squares from adding $x_3$ and $x_2$ to model containing $x_1$\n",
    "    - $SSR(x_3|x_2,x_1)$: increase in sum of squares from adding $x_3$ to model containing $x_2$ and $x_1$\n",
    "- regression sum of squares\n",
    "    - $SSR(x_1)$ is the regression sum of squares when $x_1$ is in the model\n",
    "    - $SSR(x_1,x_2,x_3)$ is the regression sum of squares when $x_1$, $x_2$, and $x_3$ are in the model\n",
    "- calculations with sequential sums of squares\n",
    "    - $SSR(x_1,x_2,x_3)=SSR(x_1)+SSR(x_2|x_1)+SSR(x_3|x_2,x_1)$\n",
    "    - $SSto = SSR(x_1,x_2,x_3) + SSE(x_1,x_2,x_3) = SSR(x_1,x_2) + SSE(x_1,x_2) = SSR(x_1) + SSE(x_1)$\n",
    "    \n",
    "```\n",
    "# sequential sum of squares\n",
    "model <- lm(yVar ~ x1Var + x2Var + x3Var)\n",
    "# outputs x1, x2|x1, and x3|x2, x1\n",
    "summary(model) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Estimates for Coefficients\n",
    "- estimates minimize $Q=\\sum_{i=i}^n(Y_i-\\beta_0-\\beta_1x_{i1}-...-\\beta_px_{ip})^2$\n",
    "    - take partial derivatives with respect to each $\\beta$\n",
    "    - set partial derivative equal to 0 and solve to find critical values\n",
    "    - check the second derivative is greater than 0 to ensure it's a minimum\n",
    "- confidence interval: $\\hat\\beta_k \\pm t_{\\alpha/2,\\;n-p-1}SE(\\hat\\beta_k)$\n",
    "    - $\\beta_0$ is mean response for $E(Y_i)$ when $x_{ij}=0$ for $j = 1,...,p$\n",
    "    - if $x_{ij}$ increases by 1 unit, the mean response $E(Y_i)$ changes by $\\beta_j$ when all predictors are held constant\n",
    "\n",
    "```\n",
    "    # method 1\n",
    "# output coefficients\n",
    "(finalModelSum <- summary(finalModel))\n",
    "# confidence interval for coefficents\n",
    "confint(finalModel, level=0.95)\n",
    "    # method 2\n",
    "# output coefficients\n",
    "finalModel(dollar)coefficients\n",
    "# standard error for coefficients\n",
    "finalModelSum(dollar)coefficients[,\"Std.Error\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Responses\n",
    "- $(\\underline{x}_0, Y_0)$ is a new response where $underline{x}_0^T=[x_{01},...,x_{0p}]$ is fixed\n",
    "    - assume $Y_0 = \\beta_0 + \\beta_1x_{01} + ... + \\beta_px_{0p} + \\epsilon_0$\n",
    "    - $Y_0 \\sim N(\\beta_0 + \\beta_1x_{il} + ... + \\beta_px_{ip}, \\sigma^2)$ and $Y_0$ independent of $Y_1,...,Y_n$\n",
    "    - $\\epsilon_0 \\sim N(0, \\sigma^2)$ and $\\epsilon_0$ independent of $\\epsilon_1,...,\\epsilon_n$\n",
    "- $\\hat{E}(Y_0) = \\hat{Y}_0 = \\hat{\\beta}_0 | \\sum_{j=1}^p\\hat{\\beta}_jx_{0j}$\n",
    "\n",
    "#### Confidence Interval for Mean Response\n",
    "- $\\hat{Y}_0 \\pm t_{\\alpha/2,\\;n-p-1}SE(\\hat{Y}_0)$\n",
    "- $SE(\\hat{Y}_0)=\\sqrt{MSE[1\\;\\underline{x_0^T}](\\textbf{X}^T\\textbf{X})^{-1}\\begin{bmatrix}1\\\\\\underline{x_0}\\end{bmatrix}}$ \n",
    "- if data is transformed so $\\tilde{Y} = ln(Y_0)$, $\\hat{E}(Y_0)$ cannot be used to estimate the mean\n",
    "    - $\\hat{E}(Y_0)$ is used to estimate the median\n",
    "    - therefore, interpret the confidence interval in terms of the median response instead of mean response\n",
    "\n",
    "```\n",
    "# mean response\n",
    "newData <- data.frame(xVar=value)\n",
    "predict(finalModel, newdata=newData, interval='confidence', level=0.95)\n",
    "```\n",
    "\n",
    "#### Prediction Interval for New Response\n",
    "- $\\hat{Y}_0 \\pm t_{\\alpha/2,\\;n-p-1}SPE(\\hat{Y}_0)$\n",
    "- $SPE(\\hat{Y}_0)=\\sqrt{MSE(1+[1\\;\\underline{x_0^T}](\\textbf{X}^T\\textbf{X})^{-1}\\begin{bmatrix}1\\\\\\underline{x_0}\\end{bmatrix})}$ \n",
    "\n",
    "```\n",
    "# new response\n",
    "newData <- data.frame(xVar=value)\n",
    "predict(finalModel, newdata=newData, interval='prediction', level=0.95)\n",
    "```\n",
    "\n",
    "#### Coefficient Interpretation\n",
    "- $E(Y_i) = \\beta_0 + \\beta_1x_{i1} + ... + \\beta_kx_{ik} + ... + \\beta_px_{ip}$\n",
    "    - increase $x_{0k}$ by $1$ unit so $\\underline{x}_0^* = [x_{01} ... x_{0k}+1 ... x_{0p}]^T$\n",
    "        - $E(Y_0)$ changes by $\\beta_k$ \n",
    "        - $E(Y_0^*) = \\beta_0 + \\beta_1x_{01} + ... + \\beta_k(x_{0k}+1) + ... + \\beta_px_{0p} = E(Y_0) + \\beta_k$\n",
    "- $E(Y_i) = \\beta_0 + \\beta_1x_{i1} + ... + \\beta_kln(x_{ik}) + ... + \\beta_px_{ip}$\n",
    "    - change $x_{0k}$ by $100\\times p$ where $p \\in (-1,1)$ so $\\underline{x}_0^* = [x_{01} ... ln((1+p)x_{0k}) ... x_{0p}]^T$\n",
    "        - $E(Y_0)$ changes by $\\beta_kln(1-p)$ \n",
    "        - $E(Y_0^*) = \\beta_0 + \\beta_1x_{01} + ... + \\beta_kln((1-p)x_{0k}) + ... + \\beta_px_{0p} = E(Y_0) + \\beta_kln(1-p)$\n",
    "     - 10 fold increase in $x_{0k}$  so $\\underline{x}_0^* = [x_{01} ... ln(10x_{0k}) ... x_{0p}]^T$\n",
    "        - $E(Y_0)$ changes by $\\beta_kln(10)$ \n",
    "        - $E(Y_0^*) = \\beta_0 + \\beta_1x_{01} + ... + \\beta_kln((10x_{0k}) + ... + \\beta_px_{0p} = E(Y_0) + \\beta_kln(10)$\n",
    "- $E(ln(Y_i)) = \\beta_0 + \\beta_1x_{i1} + ... + \\beta_kx_{ik} + ... + \\beta_px_{ip}$\n",
    "    - increase $x_{0k}$ by $1$ unit so $\\underline{x}_0^* = [x_{01} ... x_{0k}+1 ... x_{0p}]^T$\n",
    "        - median of $Y_0$ changes by a factor of $e^{\\beta_k}$ \n",
    "        - $E(ln(Y_0^*)) = \\beta_0 + \\beta_1x_{01} + ... + \\beta_k(x_{0k}+1) + ... + \\beta_px_{0p} = E(ln(Y_0)) + \\beta_k$\n",
    "        - $E(ln(Y_0^*)) \\rightarrow ln(median(Y_0^*)$ and $E(ln(Y_0)) \\rightarrow ln(median(Y_0)$\n",
    "        - then $ln(median(Y_0^*) = ln(median(Y_0) + \\beta_k \\rightarrow \\beta_k = ln(\\frac{median(Y_0^*)}{median(Y_0)}) \\rightarrow median(Y_0^*) = e^{\\beta_k}\\times median(Y_0)$\n",
    "- $E(ln(Y_i)) = \\beta_0 + \\beta_1x_{i1} + ... + \\beta_kln(x_{ik}) + ... + \\beta_px_{ip}$\n",
    "    - change $x_{0k}$ by $100\\times p$ where $p \\in (-1,1)$ so $\\underline{x}_0^* = [x_{01} ... ln((1+p)x_{0k}) ... x_{0p}]^T$\n",
    "        - median of $Y_0$ changes by a factor of $(1-p)^{\\beta_k}$\n",
    "        - $E(ln(Y_0^*)) = \\beta_0 + \\beta_1x_{01} + ... + \\beta_kln((1-p)x_{0k}) + ... + \\beta_px_{0p} = E(ln(Y_0)) + \\beta_kln(1-p)$\n",
    "        - $E(ln(Y_0^*)) \\rightarrow ln(median(Y_0^*)$ and $E(ln(Y_0)) \\rightarrow ln(median(Y_0)$\n",
    "        - then $ln(median(Y_0^*) = ln(median(Y_0) + \\beta_kln(1-p) \\rightarrow \\beta_kln(1-p) = ln(\\frac{median(Y_0^*)}{median(Y_0)}) \\rightarrow median(Y_0^*) = (1-p)^{\\beta_k}\\times median(Y_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "### Polynomial Regression Models\n",
    "#### Single Predictor\n",
    "$E(Y_i)=\\beta_0+\\beta_1x_i+\\beta_2x_i^2+...+\\beta_hx_i^h$\n",
    "\n",
    "```\n",
    "# linear model with single predictor\n",
    "linearLM <- lm(yVar ~ xVar)\n",
    "# quadratic model with single predictor\n",
    "quadraticLM <- <- lm(yVar ~ xVar + I(xVar^2))\n",
    "# cubic model with single predictor\n",
    "cubicLM <- lm(yVar ~ xVar + I(xVar^2) + I(xVar^3))\n",
    "```\n",
    "\n",
    "#### Multiple Predictors\n",
    "$E(Y_i)=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\beta_{11}x_{i1}^2+\\beta_{22}x_{i2}^2+\\beta_{12}x_{i1}x_{i2}$\n",
    "```\n",
    "# quadratic model with multiple predictors\n",
    "multQuadLM <- lm(yVar ~ xVar1 + I(xVar1^2) + xVar2 + I(xVar2^2) + xVar1:xVar2)\n",
    "# cubic model with multiple predictors\n",
    "multCubeLM <- lm(yVar ~ xVar1+ I(xVar1^2) + I(xVar1^3) + xVar2 + I(xVar2^2) + I(xVar2^3) + xVar1:xVar2 + \n",
    "                I(xVar1^2):xVar2 + I(xVar2^2):xVar1)\n",
    "```\n",
    "\n",
    "#### Hierarchy Principle\n",
    "- if $x^h$ is in the model, then the model must include all $x^j$ for $0 \\leq j \\leq h$, whether or not lower order terms are significant\n",
    "\n",
    "### Polynomial Regression Hypothesis Tests\n",
    "#### Single Predictor\n",
    "Can the degree of the model be reduced?\n",
    "- $H_0: \\beta_3=0$ vs $H_a:\\beta_3\\neq0$\n",
    "\n",
    "```\n",
    "# single predictor polynomial hypothesis test\n",
    "cubicNeededTest <- anova(quadraticLM, cubicLM)\n",
    "```\n",
    "\n",
    "#### Multiple Predictors\n",
    "Does the first predictor have an effect?\n",
    "- $H_0: \\beta_1=\\beta_{11}=\\beta_{12}=0$ vs $H_a:$ at least one $\\beta_j\\neq0$ for $j=1,\\;11,\\;12$\n",
    "\n",
    "```\n",
    "# reduced model\n",
    "secondPredOnlyLM <- lm(yVar ~ xVar2 + I(xVar2^2))\n",
    "# test for effect of first predictor\n",
    "anvoa(secondPredOnlyLM, multQuadLM)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "### Encoding Categorical Variables\n",
    "```\n",
    "# encoding categorical variables\n",
    "    # method 1\n",
    "x2 <- ifelse(as.character(data(dollar)catVar) == 'val1', 1, 0)\n",
    "x3 <- ifelse(as.character(data(dollar)catVar) == 'val2', 1, 0)\n",
    "    # method 2, best if have multiple categorical variables\n",
    "catLM <- lm(yVar ~ as.factor(catVar), data=data)\n",
    "catMat <- model.matrix(catLM)\n",
    "```\n",
    "### Parallel Model\n",
    "#### Equation\n",
    "$E(Y_i)=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\beta_3x_{i3}$\n",
    "- predictor: $x_{i1}$\n",
    "- categorical predictors: $x_{i2},\\;x_{i3}$\n",
    "\n",
    "#### Categorical Variables\n",
    "- treatments: $A_i=(1,\\;0),\\;B_i=(0,\\;1),\\;C_i=(0,\\;0)$\n",
    "    - $\\begin{equation}x_{i2}=\\begin{cases}1 &\\text{if in A}\\\\0 &\\text{otherwise} \\end{cases}\\end{equation}$\n",
    "    - $\\begin{equation}x_{i3}=\\begin{cases}1 &\\text{if in B}\\\\0 &\\text{otherwise} \\end{cases}\\end{equation}$\n",
    "- first order categorical means:\n",
    "    - A: $\\beta_0 + \\beta_2 + \\beta_1x_{i1}$\n",
    "    - B: $\\beta_0 + \\beta_3 + \\beta_1x_{i1}$\n",
    "    - C: $\\beta_0 + \\beta_1x_{i1}$\n",
    "    \n",
    "#### Coefficient Interpretation\n",
    "- $\\beta_1$ = slope for predictor after controlling for categorical variable\n",
    "- $\\beta_0$ = mean for C for any predictor value\n",
    "- $\\beta_2$ = mean difference between A and C for any predictor value\n",
    "- $\\beta_3$ = mean difference between B and C for any predictor value\n",
    "    \n",
    "```\n",
    "# parallel model\n",
    "parallelLM <- lm(yVar ~ xVar + as.factor(catVar))\n",
    "# matrix to show which categorical values correspond to each coefficient\n",
    "model.matrix(parallelLM)\n",
    "```\n",
    "\n",
    "### Nonparallel Model\n",
    "#### Equation\n",
    "$E(Y_i)=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\beta_3x_{i3}+\\beta_{12}x_{i1}x_{i2}+\\beta_{13}x_{i1}x_{i3}$\n",
    "- predictor: $x_{i1}$\n",
    "- categorical predictors: $x_{i2},\\;x_{i3}$\n",
    "\n",
    "#### Categorical Variables\n",
    "- treatments: $A_i=(1,\\;0),\\;B_i=(0,\\;1),\\;C_i=(0,\\;0)$\n",
    "    - $\\begin{equation}x_{i2}=\\begin{cases}1 &\\text{if in A}\\\\0 &\\text{otherwise} \\end{cases}\\end{equation}$\n",
    "    - $\\begin{equation}x_{i3}=\\begin{cases}1 &\\text{if in B}\\\\0 &\\text{otherwise} \\end{cases}\\end{equation}$\n",
    "- second order categorical means: \n",
    "    - A: $\\beta_0 + \\beta_2 + (\\beta_1+\\beta_{12})x_{i1}$\n",
    "    - B: $\\beta_0 + \\beta_3 + (\\beta_1+\\beta_{13})x_{i1}$\n",
    "    - C: $\\beta_0 + \\beta_1x_{i1}$\n",
    "    \n",
    "#### Coefficient interpretation\n",
    "- $\\beta_2$ are $\\beta_3$ differences in intercepts\n",
    "- $\\beta_{12}$ are $\\beta_{13}$ differences in slopes\n",
    "\n",
    "```\n",
    "# nonparallel model\n",
    "nonparallelLM <- lm(yVar ~ xVar + as.factor(catVar) + xVar*as.factor(catVar))\n",
    "# matrix to show which categorical values correspond to each coefficient\n",
    "model.matrix(nonparallelLM)\n",
    "```\n",
    "\n",
    "### Hypothesis Tests for Models with Categorical Variables\n",
    "Is there a significant linear relationship between predictors and responses for all groups of the categorical variable?\n",
    "- $H_0:\\beta_1=0$ vs $H_a:\\beta_a\\neq0$\n",
    "\n",
    "```\n",
    "# reduced model\n",
    "catOnlyLM <- lm(yVar ~ as.factor(catVar))\n",
    "# test for linear relationship\n",
    "anova(catOnlyLM, parallelLM)\n",
    "anova(catOnlyLM, nonparallelLM)\n",
    "```\n",
    "\n",
    "Is there a significant difference in the mean response of treatments A, B, and C for any predictor?\n",
    "- parallel\n",
    "    - $\\beta_{2}=\\beta_{3}=0$ vs $H_a:$ at least one $\\beta_{j}\\neq0$ for $j=2,\\;3$\n",
    "- nonparallel\n",
    "    - $\\beta_2=\\beta_3=\\beta_{12}=\\beta_{13}=0$ vs $H_a:$ at least one $\\beta_{j}\\neq0$ for $j=2,\\;3,\\;12,\\;13$\n",
    "\n",
    "```\n",
    "# reduced model\n",
    "noCatLM <- lm(yVar ~ xVar)\n",
    "# test for difference in mean response\n",
    "anova(noCatLM, parallelLM)\n",
    "anova(noCatLM, nonparallelLM)\n",
    "```\n",
    "\n",
    "Is there a significant interaction term between the predictor and categorical variables?\n",
    "- $\\beta_{12}=\\beta_{13}=0$ vs $H_a:$ at least one $\\beta_{1j}\\neq0$ for $j=2,\\;3$\n",
    "\n",
    "```\n",
    "# test for interaction effect\n",
    "anova(parallelLM, nonparallelLM)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
