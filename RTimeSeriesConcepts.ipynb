{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Coding Concepts for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Data\n",
    "- a series of observations taken sequentially over time\n",
    "- obervations are independent, become available at equally spaced time points, and are independent\n",
    "\n",
    "### ARMA Model Simulation\n",
    "$X_{t} = 0.75X_{t-1} - 0.5X_{t-2} + Z_{t} + 1.2Z_{t-1}$\n",
    "``` {r}\n",
    "# simulate an ARMA model\n",
    "arma21 <- arima.sim(model = list(ar = c(0.75, -0.5), ma = 1.2, sd = 1), n = 200)\n",
    "plot(arma21, xlab='Time', ylab=expression(X[t]), type='l', main='ARMA(2, 1)')\n",
    "# add trend line\n",
    "t <- 1:length(arma21)\n",
    "fit <- lm(arma21 ~ t)\n",
    "abline(fit, col = 'red')\n",
    "# add mean line\n",
    "m <- mean(arma21)\n",
    "abline(h = m, col = 'blue')\n",
    "```\n",
    "\n",
    "### Import Data\n",
    "```\n",
    "# read data\n",
    "data <- read.table(\"path/to/file/fileName.csv\", header=TRUE)\n",
    "```\n",
    "\n",
    "### Load Data Already Installed in R\n",
    "```\n",
    "# load data\n",
    "data(\"dataName\")\n",
    "```\n",
    "\n",
    "### Create Time Series Object\n",
    "```\n",
    "# create time series object for monthly data\n",
    "dataTS <- ts(data[,1], start=c(YEAR, MONTH), frequency=12)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Calculations\n",
    "### Yule Walker Equations\n",
    "- $\\rho_x(0)=1$\n",
    "- for $h \\geq 1:$\n",
    "    - $h=1:\\; \\phi_{n1} + \\phi_{n2}\\rho_x(1) + ... + \\phi_{nn}\\rho_x(n-1) = \\rho_x(1)$\n",
    "    - $h=2:\\; \\phi_{n1}\\rho_x(1) + \\phi_{n2} + ... + \\phi_{nn}\\rho_x(n-2) = \\rho_x(2)$\n",
    "    - $\\vdots$\n",
    "    - $h=n:\\; \\phi_{n1}\\rho_x(n-1) + \\phi_{n2}\\rho_x(n-2) + ... + \\phi_{nn} = \\rho_x(n)$\n",
    "    \n",
    "### Autocovariance Function (ACVF)\n",
    "- $\\gamma_x(t,\\;s)=Cov(X_t,\\;X_s)=E(X_tX_s)-E(X_t)E(X_s)$\n",
    "- describes the linear strength between $X_t$ and $X_s$\n",
    "- for stationary data, $\\gamma_x(0) = \\sigma_x^2$\n",
    "    - $\\gamma_x(k) = \\gamma_x(-k)$\n",
    "    \n",
    "ACVF Matrix <br>\n",
    "$\\begin{equation}\\Gamma_p = \\begin{bmatrix}\n",
    "\\gamma(0) & \\gamma(1) & \\dots & \\gamma(p-1)\\\\\n",
    "\\gamma(1) & \\gamma(0) & \\dots & \\gamma(p-2)\\\\\n",
    "\\vdots & \\vdots  & \\ddots & \\vdots\\\\\n",
    "\\gamma(p-1) & \\gamma(p-2) & \\dots & \\gamma(0)\n",
    "\\end{bmatrix} \\end{equation}$\n",
    "\n",
    "### Autocorrelation Function (ACF)\n",
    "- $\\rho_x(t,x)=Corr(X_t,\\;X_s)=\\frac{Cov(X_t,\\;X_s}{\\sqrt{Var(X_t)Var(X_s)}}=\\frac{\\gamma_x(t,\\;s)}{\\sigma_x(t)\\sigma_x(s)}$ \n",
    "- for stationary data, $\\rho_x(k) = \\frac{\\gamma_x(k)}{\\gamma_x(0)}$\n",
    "    - $\\rho_x(k)=\\rho_x(-k)$\n",
    "\n",
    "### Partial Autocorrelation Function (PACF)\n",
    "- $\\alpha(0) = 1,\\;\\alpha(n)=\\;\\phi_{nn}=n^{th}$ partial autocorrelation\n",
    "- gives the partial autocorrelation of a stationary time series with its own lagged values while controlling for other lags\n",
    "- solve the system of Yule-Walker equations to find the pacfs: $\\underline{\\phi}_n = \\textbf{R}_n^{-1}\\underline{\\rho}_n$ <br>\n",
    "$\\begin{bmatrix}\n",
    "1 & \\rho_x(1) & \\dots & \\rho_x(n-1)\\\\\n",
    "\\rho_x(1) & 1 & \\dots & \\rho_x(n-2)\\\\\n",
    "\\vdots & \\vdots  & \\ddots & \\vdots\\\\\n",
    "\\rho_x(n-1) & \\rho_x(n-2) & \\dots & 1\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\phi_{n1}\\\\\n",
    "\\phi_{n2}\\\\\n",
    "\\vdots\\\\\n",
    "\\phi_{nn}\\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "\\rho_x(1)\\\\\n",
    "\\rho_x(2)\\\\\n",
    "\\vdots\\\\\n",
    "\\rho_x(n)\\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Original Data\n",
    "### Time Series Plot of Original Data\n",
    "Examine graph for trend, seasonality, change of variance, and sharp behavior.\n",
    "```\n",
    "# plot original data\n",
    "ts.plot(dataTS, xlab='time units', ylab='y units', main='Original Data')\n",
    "# add trend line\n",
    "t <- 1:length(dataTS)\n",
    "fit <- lm(dataTS ~ t)\n",
    "abline(fit, col = 'red')\n",
    "# add mean line\n",
    "m <- mean(dataTS)\n",
    "abline(h = m, col = 'green')\n",
    "# add a legend\n",
    "legend('topleft, c('mean line', 'trend line'), fill=c('green', 'red'))\n",
    "```\n",
    "\n",
    "### Histogram of Original Data\n",
    "Examine histogram for bell shaped curve, indicating normality. \n",
    "```\n",
    "# histogram of original data\n",
    "hist(dataTS, ylab='y units', main='Histogram of Original Data')\n",
    "```\n",
    "\n",
    "### Autocorrelation and Partial Autocorrelation Plots\n",
    "Moment Estimates for Stationary Time Series\n",
    "- sample mean: $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^nX_t$\n",
    "- sample variance: $\\hat{\\sigma}_x^2 = \\hat{\\gamma}_x(0) = \\frac{1}{n}\\sum_{t=1}^n(X_t-\\bar{X}_n)^2$\n",
    "- sample acvf at lag h: $\\hat{\\gamma}_x(h) = \\frac{1}{n}\\sum_{t=1}^{n-h}(X_t-\\bar{X}_n)(X_{t+h}-\\bar{X}_n)$\n",
    "- sample acf at lag h: $\\rho_x(h)=\\frac{\\rho_x(h)}{\\rho_x(0)}$\n",
    "\n",
    "Sample PACF Yule Walker Calculation <br>\n",
    "1: calculate the sample acfs $\\underline{\\hat{\\rho}}_h = <\\hat{\\rho}(1), ...,\\;\\hat{\\rho}(h)>$ <br>\n",
    "2: replace unknown $\\underline{\\rho}_h$ by sample acf in Yule Walker equations to get $\\hat{\\textbf{R}}_h\\underline{\\hat{\\phi}}_h=\\underline{\\hat{\\rho}}_h$ <br>\n",
    "3: $\\hat{\\alpha}(h)=\\hat{\\phi}_{hh}$ is the last component of $\\underline{\\hat{\\phi}}_h=\\hat{\\textbf{R}}_h^{-1}\\underline{\\hat{\\rho}}_h$\n",
    "\n",
    "Sample PACF Durbin-Levinson Algorithm\n",
    "- used to recursively calculate the sample pacf\n",
    "- $\\hat\\phi_{hh}=\\frac{\\hat\\rho(h)-\\sum_{j=1}^{h-1}\\hat\\phi_{h-1,j}\\hat\\rho(h-j)}{1-\\sum_{j=1}^{h-1}\\hat\\phi_{h-1,j}\\hat\\rho(j)}$\n",
    "- $\\hat\\phi_{h,j}=\\hat\\phi_{h-1,j}-\\hat\\phi_{hh}\\hat\\phi_{h-1,h-j}$\n",
    "\n",
    "Model Identification\n",
    "- if $|\\hat{\\rho}(h_0)| > 1.96n^{-1/2}$ and $|\\hat{\\rho}(h)| < 1.96n^{-1/2}$ for all $h > h_0$, assume MA(q) with $q=h_0$\n",
    "    - the confidence interval containing 0: $0 \\pm 1.96n^{-1/2}(1+2\\sum_{k=1}^q[\\hat\\rho(k)]^2)^{1/2}$\n",
    "    - R drops the $2\\sum_{k=1}^q[\\hat\\rho(k)]^2$ term, so assume $\\hat{\\rho}(h)$ is in the confidence interval if $|\\hat{\\rho}(h)| \\approx 1.96n^{-1/2}$\n",
    "- if $|\\hat{\\alpha}(h_0)| > 1.96n^{-1/2}$ and $|\\hat{\\alpha}(h)| < 1.96n^{-1/2}$ for all $h > h_0$, assume AR(p) with $p=h_0$\n",
    "    - for large n, sample pacfs at lags $h > p$ are $\\alpha(h) \\overset{iid}{\\sim} N(0,\\;\\frac{1}{n})$\n",
    "\n",
    "```\n",
    "# acf of original data\n",
    "acf(dataTS, lag.max=60, main = 'ACF of Original Data')\n",
    "# pacf of original data\n",
    "pacf(dataTS, lag.max=60, main = 'PACF of Original Data')\n",
    "```\n",
    "\n",
    "### Variance of Original Data\n",
    "\n",
    "```\n",
    "# variance of original data\n",
    "var(dataTS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Stationary\n",
    "- data is stationary if the mean, variance, and autocorrelation structure do not change over time\n",
    "    - $E(X_t)=\\mu$ for all $t$\n",
    "    - $Var(X_t)=\\sigma_x^2$ for all $t$\n",
    "    - $E(X_t^2)<\\infty$ for all $t$\n",
    "    - $\\gamma_x(t,s)=\\gamma_x(t+r,s+r)=\\gamma_x(s-t)=\\gamma_x(k)$\n",
    "- graphs of stationary data have no trend, seasonality, change of variance, or sharp behavior\n",
    "    - over two equal length time intervals, the graph should exhibit similar characteristics\n",
    "\n",
    "### Remove Last Five Observations for Forecasting\n",
    "```\n",
    "# remove last five data points\n",
    "len <- length(dataTS)\n",
    "modLen <- len - 5\n",
    "modelData <- dataTS[1:modLen]\n",
    "```\n",
    "\n",
    "### Stabilize Variance\n",
    "Box-Cox Transformation\n",
    "- choose a round number for $\\hat{\\lambda}$ like $-1, 0, 1, 0.5, ...$ since round numbers are easier to transform back to original units\n",
    "    - $\\hat{\\lambda} = 1$ means no transformation is needed to stabilize the variance\n",
    "    - $\\hat{\\lambda} = 0$ means a log transformation is needed to stabilize the variance\n",
    "- try several transformations suggested by the confidence interval\n",
    "\n",
    "```\n",
    "library(MASS)\n",
    "# find lambda for box-cox\n",
    "t <- 1:length(modelData)\n",
    "bcTransform <- boxcox(modelData ~ t, plotit = TRUE)\n",
    "lambda <- bcTransform(dollar)[which(bcTransform(dollar)y == max(bcTransform(dollar)y))]\n",
    "# box-cox transformation\n",
    "dataBC <- (1/lambda) * (modelData^lambda - 1)\n",
    "# log transformation\n",
    "dataLog <- log(modelData)\n",
    "# square root transformation\n",
    "dataSqrt <- sqrt(modelData)\n",
    "```\n",
    "\n",
    "### Differencing\n",
    "- lag $s$ difference: $\\nabla_sX_t=(1-B^s)X_t=X_t-X_{t-s}$\n",
    "- $D^{th}$ difference at lag $s$: $\\nabla_s^DX_t=(1-B^s)^DX_t$\n",
    "- $d^{th}$ difference at lag $1$: $\\nabla^dX_t={d\\choose0}X_t-{d\\choose1}X_{t-1}+{d\\choose2}X_{t-2}+ ... + (-1)^d{d\\choose d}X_{t-d}$\n",
    "\n",
    "### Difference to Remove Seasonality\n",
    "- difference $D$ times at lag $s$ to remove seasonality\n",
    "\n",
    "```\n",
    "# difference at lag 12 to remove seasonal component\n",
    "diffSeas <- diff(dataBC, 12)\n",
    "```\n",
    "\n",
    "### Difference to Remove Trend\n",
    "- difference $d$ times at lag $1$ to remove trend\n",
    "\n",
    "```\n",
    "# difference at lag 1 to remove trend\n",
    "diffTrend <- diff(diffSeas, 1)\n",
    "```\n",
    "\n",
    "### Unit Roots and Differencing\n",
    "Overdifferenced\n",
    "- unit root in MA part\n",
    "    - $\\phi(B)X_t=\\theta^*(B)Z_t$ with $\\theta^*(z)=(1-z)\\theta(z)$\n",
    "    - $Y_t$ is an invertible ARMA process such that $\\phi(B)Y_t=\\theta(B)Z_t$ \n",
    "    - let $X_t = \\nabla Y_t = Y_t-Y_{t-1}$\n",
    "    - differencing an invertible ARMA process produces a unit root in the MA part since $\\phi(B)X_t = \\phi(B)(1-B)Y_t = (1-B)\\theta(B)Z_t$\n",
    "- the variance of an overdifferenced process is larger than the variance of the original process\n",
    "    - if additional differencing increases the variance, it is unnecessary\n",
    "\n",
    "Underdifferenced\n",
    "- unit root in AR part\n",
    "    -  $\\phi^*(B)X_t=\\theta(B)Z_t$ with $\\phi^*(z) = (1-z)\\phi(z)$\n",
    "    - let $W_t=X_t-X_{t-1}$ so $\\phi(B)W_t=\\theta(B)Z_t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "Seasonal Autoregressive Integrated Moving Average Model\n",
    "- $SARIMA(p,d,q)\\times(P,D,Q)_s:$ $\\phi(B)\\Phi(B^s)(1-B)^d(1-B^s)^DX_t=\\theta(B)\\Theta(B)Z_t$\n",
    "    - $\\phi(B) = 1 - \\phi_1B - ... - \\phi_pB^p$\n",
    "    - $\\Phi(B^s) = 1 - \\Phi_1B^s - ... - \\Phi_PB^{sP}$\n",
    "    - $\\theta(B) = 1 + \\theta_1B + ... + \\theta_qB^q$\n",
    "    - $\\Theta(B^s) = 1 + \\Theta_1B + ... + \\Theta_QB^{sQ}$\n",
    "\n",
    "Shift Operator\n",
    "- $BX_t = X_{t-1}$\n",
    "- $B^kX_t=X_{t-k}$\n",
    "\n",
    "SARIMA as ARMA\n",
    "- $Y_t=(1-B)^d(1-B^s)^DX_t$ is an ARMA(p+sP, q+sQ) process\n",
    "\n",
    "### Identify p, q, P, and Q\n",
    "- for P and Q: examine acf and pacf graphs at lags which are multiples of s to identify ARMA(P, Q)\n",
    "- for p and q: examine acf and pacf graphs for lags 0 to s to identify ARMA(p, q)\n",
    "\n",
    "Behavior of ACF and PACF Graphs for Pure SARIMA Models\n",
    "- values are 0 at nonseasonal lags h $\\neq$ ks for k = 1, 2, ... \n",
    "- tails off means data may decay exponentially, oscillate, or have sinusoidal behavior\n",
    "- SAR(P): acf tails off, pacf cuts off after lag sP\n",
    "- SMA(Q): acf cuts off after lag sQ, pacf tails off\n",
    "- SARMA(P, Q): acf tails off at lag sk, pacf tails off at lag sk\n",
    "\n",
    "Behavior of ACF and PACF Graphs for ARMA Models\n",
    "- tails off means data may decay exponentially, oscillate, or have sinusoidal behavior\n",
    "- AR(p): acf tails off, pacf cuts off after lag p\n",
    "- MA(q): acf cuts off after lag q, pacf tails off\n",
    "- ARMA(p, q): acf tails off, pacf tails off\n",
    "\n",
    "\n",
    "### Compare Different Models\n",
    "- choose the simpliest model with the lowest Akaike Information Criterion Corrected for Bias (AICC)\n",
    "    - evaluates models based on goodness of fit and model complexity\n",
    "- $AICC=-2ln(L(\\underline{\\theta_q},\\underline{\\phi_p},\\frac{S(\\underline{\\theta_q},\\underline{\\phi_p})}{n}))+\\frac{2n(p+q+1)}{n-p-q-2}$\n",
    "\n",
    "```\n",
    "library(qpcR)\n",
    "# for loop to create different models\n",
    "diffAICC <- NULL\n",
    "for (p in 0:e){\n",
    "    for (q in 0:f){\n",
    "        for (P in 0:g){\n",
    "            for (Q in 0:h){\n",
    "                mod <- arima(modelData, order=c(p,d,q), seasonal=list(P,D,Q), period=s, method=\"ML\")\n",
    "                aicc <- AICc(mod)\n",
    "                row <- c(p,d,q,P,D,Q,aicc)\n",
    "                diffAICC <- rbind(diffAICC, row)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# output AICCs for each model\n",
    "(ordered <- diffAICC[order(diffAICC[,7], decreasing=FALSE),])\n",
    "```\n",
    "\n",
    "### Preliminary Coefficent Estimates\n",
    "- moment estimates find the correct region for optimization\n",
    "    - optimization finds the local extrema of a function\n",
    "    - the correct region of optimization is needed to obtain absolute extrema of a function\n",
    "- then least squares or maximum likelihood estimators optimize the process\n",
    "\n",
    "#### Yule Walker \n",
    "- use sample acf in Yule Walker equations to find estimates\n",
    "    - $\\underline{\\hat\\phi_p}=\\textbf{R}_p^{-1}\\underline{\\hat\\rho_p}$\n",
    "    - $\\hat\\sigma_x^2=\\hat\\gamma(0)$\n",
    "    - $\\hat\\sigma_z^2=\\hat\\nu_h=\\hat\\gamma(0)[1-\\underline{\\hat\\phi_{p}}'\\underline{\\hat\\rho_p}]=\\hat\\gamma(0)-\\underline{\\hat\\phi_h}'\\underline{\\hat\\gamma_h}$\n",
    "\n",
    "Durbin-Levinson Algorithm\n",
    "- $\\hat\\phi_{hh}=[\\hat\\gamma(h)-\\sum_{j=1}^{h-1}\\hat\\phi{h-1,j}\\hat\\gamma(h-j)]\\hat\\nu_{h-1}^{-1}$\n",
    "- $\\hat\\phi_{h,j}=\\hat\\phi_{h-1,j}-\\hat\\phi_{hh}\\hat\\phi_{h-1,h-j}$\n",
    "- $\\hat\\nu_h=\\hat\\nu_{h-1}[1-\\hat\\phi_{hh}^2]$\n",
    "    - $\\hat{\\phi}_{11} = \\hat{\\rho}(1) = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)}$\n",
    "    - $\\hat{\\nu}_0=\\hat{\\gamma}(0)$\n",
    "\n",
    "Distribution of Yule Walker Estimates\n",
    "- $\\underline{\\hat{\\phi}}_p \\sim N(\\underline{\\phi}_p,\\;n^{-1}\\sigma_z^2\\Gamma_p^{-1})$\n",
    "- Confidence Interval: $\\hat{\\phi}_{pj} \\pm 1.96n^{-1/2}\\hat{\\nu}_{jj}^{1/2}$\n",
    "    - $\\hat{\\nu}_{jj}$ is the $j^{th}$ diagonal element of $\\hat{\\nu}_p\\hat{\\Gamma}_p^{-1}$\n",
    "    - $\\hat{\\nu}_p$ is an estimate for $\\sigma_z^2$ for AR(p)\n",
    "\n",
    "PACF and AR(p) Coefficients\n",
    "- $\\hat{\\phi}_{p1} = \\hat{\\phi}_1,\\;\\hat{\\phi}_{p2} = \\hat{\\phi}_2,...,\\;\\hat{\\phi}_{pp} = \\hat{\\phi}_p,$\n",
    "\n",
    "```\n",
    "    # first method\n",
    "yw <- ar.yw(modelData, order=p)\n",
    "# parameter estimates\n",
    "yw(dollar)ar\n",
    "    # second method\n",
    "(fit <- ar(modelData, method='yule-walker'))\n",
    "```\n",
    "\n",
    "#### Innovation Algorithm\n",
    "Expected Values\n",
    "- $E(X_j|X_1,...,X_n)=X_j$ if $1 \\leq j \\leq n$\n",
    "- $E(Z_j|X_1,...,X_n)=Z_j$ if $1 \\leq j \\leq n$\n",
    "- $E(Z_j|X_1,...,X_n)=0$ if $j > n$\n",
    "\n",
    "Innovation\n",
    "- $Z_{n+1} = X_{n+1} - \\hat{X}_{n+1}$\n",
    "    - $X_{n+1}=\\phi_1X_n + ... + \\phi_pX_{n+1-p} + \\theta_1Z_n + ... + \\theta_qZ_{n+1-q} + Z_{n+1}$\n",
    "    - $\\hat{X}_{n+1} = E(\\phi_1X_n + ... + \\phi_pX_{n+1-p} + \\theta_1Z_n + ... + \\theta_qZ_{n+1-q} + Z_{n+1}|X_1,...,X_n) = \\phi_1X_n + ... + \\phi_pX_{n+1-p} + \\theta_1Z_n + ... + \\theta_qZ_{n+1-q}$\n",
    "    \n",
    "Innovation Algorithm\n",
    "- applicable to all time series with a finite variance \n",
    "- assume $E(X_t)=0,\\;E(X_tX_s)=\\kappa(t,s),\\;E(|X_t|^2) < \\infty$\n",
    "    - $\\kappa(t,s)=\\gamma(s-t)$ for stationary time series\n",
    "- $\\hat{X}_{n+1}=\\sum_{j=1}^n\\theta_{nj}(X_{n+1-j}-\\hat{X}_{n+1-j})$\n",
    "    - when $j=n$, $\\hat{X}_{n+1-j}=\\hat{X}_1=E(X_1)=0$\n",
    "- compute $\\theta_{n1},...,\\theta_{nn}$ from equations where $\\nu_0=\\kappa(1,1)$\n",
    "    - $\\theta_{n,n-k} = [\\kappa(n+1,k+1) - \\sum_{j=0}^n\\theta_{k,k-j}\\theta_{n,n-j}\\nu_j]\\nu_k^{-1}$ where $0 \\leq k < n$\n",
    "    - $\\nu_n = \\kappa(n+1,n+1) - \\sum_{j=0}^{n-1}\\theta_{n,n-j}^2\\nu_j$\n",
    "- solve: $\\nu_0 = \\kappa(1,1) = \\gamma(0) \\rightarrow \\hat{\\theta}_{11}(n=1,\\;k=0),\\;\\nu_1 \\rightarrow \\hat{\\theta}_{22}(n=2,\\;k=0),\\; \\hat{\\theta}_{21}(n=2,\\;k=1),\\; \\nu_2 \\rightarrow \\hat{\\theta}_{33}(n=3,\\;k=0), \\;\\hat{\\theta}_{32}(n=3,\\;k=1), \\;\\hat{\\theta}_{31}(n=3,\\;k=2), \\; \\nu_3 \\rightarrow$ \n",
    "- $\\nu_n=E[(X_{n+1}-\\hat{X}_{n+1})^2]$ is the innovation variance\n",
    "\n",
    "```\n",
    "# download program for innovations\n",
    "source(\"innovations.r\")\n",
    "# find acvf\n",
    "acvf = acf(modelData, plot=FALSE, lag.max=length(LakeHuron))(dollar)acf * var(LakeHuron)\n",
    "m = length(acvf)\n",
    "# find innovations\n",
    "innov <- innovations.algorithm(m+1, acvf)\n",
    "# estimates for MA(q)\n",
    "innov(dollar)thetas[q, 1:q]\n",
    "```\n",
    "\n",
    "### Maximum Likelihood Coefficient Estimates\n",
    "- assume $X_t$ is causal and invertible and $Z_t \\sim IID(0, \\sigma_z^2)$ <br>\n",
    "\n",
    "1: start with preliminary estimates from innovation algorithm where $X_j-\\hat{X}_j=Z_j\\sim N(0, v_{j-1} = \\sigma_z^2r_{j-1}$ <br>\n",
    "\n",
    "2: use Gaussian likelihood function $L(\\underline{\\theta},\\;\\underline{\\phi},\\;\\sigma_z^2) = \\frac{1}{\\sqrt{(2\\pi\\sigma_z^2)^nr_0\\times ... \\times r_{n-1}}}exp[-\\frac{1}{2\\sigma_z^2}\\sum_{j=1}^n\\frac{1}{r_{j-1}}(X_j-\\hat{X}_j)^2]$\n",
    "   - let $S_x(\\underline{\\beta})=\\sum_{j=1}^n\\frac{1}{r_{j-1}}(X_j-\\hat{X}_j)^2$ and $\\underline{\\beta} = <\\underline{\\theta}_q,\\;\\underline{\\phi}_p>$ <br>\n",
    "    \n",
    "3: minimize the negative loglikelihood $-l(\\underline{\\beta}, \\sigma_z^2)$ to find values of $<\\underline{\\beta}, \\sigma_z^2>$ that maximize the likelihood \n",
    "   - $(\\underline{\\hat{\\theta}},\\;\\underline{\\hat{\\phi}})$ are the values that minimize the reduced likelihood $l(\\beta)=ln(n^{-1}S(\\underline{\\beta})) + \\frac{1}{n}\\sum_{j=1}^nln(r_{j-1})$\n",
    "       - $r_n=\\frac{\\nu_n}{\\sigma_z^2}$\n",
    "   - $\\hat{\\sigma}_z^2 = n^{-1}S(\\underline{\\hat{\\theta}},\\;\\underline{\\hat{\\phi}}) = \\frac{1}{n}\\sum_{j=1}^n\\frac{1}{r_{j-1}}(X_j-\\hat{X}_j)^2$\n",
    "       - $\\hat{X}_j=E(X_j|X_1,\\;...,\\;X_{j-1})$ are the one step ahead predictors of $X_j$'s given by the innovation algorithm\n",
    "\n",
    "```\n",
    "# MLE for sarima with no trend\n",
    "mod <- arima(modelData, order=c(p,d,q), seasonal=list(P,D,Q), period=s, method=\"ML\")\n",
    "# MLE for sarima with trend\n",
    "mod <- arima(modelData, order=c(p,d,q), seasonal=list(P,D,Q), period=s, method=\"ML\", \n",
    "            xreg=1:length(modelData)-d)\n",
    "```\n",
    "### Confidence Intervals for Maximum Likelihood Estimators\n",
    "- $\\beta_j \\pm 1.96n^{-1/2}v_{jj}^{1/2}(\\underline{\\hat{\\beta}})$\n",
    "    - $v_{jj}$'s are known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Checking\n",
    "\n",
    "### Check Model is Stationary and Invertible\n",
    "- Maximum Likelihood method assumes data is stationary and invertible\n",
    "    - SARIMA models do not adjust for changes in expected value and variance, so the data must be stationary\n",
    "    - invertibility means observations in the recent past have a larger impact on the current shock \n",
    "    \n",
    "Causal and Stationary\n",
    "- {$X_t$} is causal if {$X_t$} can be expressed as a converging series of values of {$Z_t$}  \n",
    "- $X_t$ is causal iff $\\phi(z)=1-\\phi_1z-...-\\phi_pz^p \\neq 0$ for all $|z| \\leq 1$\n",
    "    - ensure all roots of $\\phi(z)$ are greater than 1\n",
    "- $X_t = \\frac{1}{\\phi(B)}Z_t=\\sum_{j=0}^\\infty\\psi_jB^jZ_t=\\sum_{j=0}^\\infty\\psi_jZ_{t-j}$ where $\\sum_{j=0}^\\infty|\\psi_j| < \\infty$ \n",
    "\n",
    "Invertible\n",
    "- {$X_t$} is invertible if {$Z_t$} can be expressed as a converging series of values of {$X_t$}  \n",
    "- $X_t$ is invertible iff $\\theta(z)=1+\\theta_1z-...+\\theta_qz^q \\neq 0$ for all $|z| \\leq 1$\n",
    "    - ensure all roots of $\\theta(z)$ are greater than 1\n",
    "- $X_t = \\frac{1}{\\theta(B)}X_t=\\sum_{j=0}^\\infty\\pi_jB^jX_t=\\sum_{j=0}^\\infty\\pi_jX_{t-j}$ where $\\sum_{j=0}^\\infty|\\pi_j| < \\infty$ \n",
    "\n",
    "```\n",
    "# output roots of AR part\n",
    "polyroot(c(phi_1, ..., phi_p))\n",
    "# output roots of MA part\n",
    "polyroot(c(theta_1, ..., theta_q))\n",
    "# output roots of SAR part\n",
    "polyroot(c(PHI_1, ..., PHI_P))\n",
    "# output roots of SMA part\n",
    "polyroot(c(THETA_1, ..., THETA_Q))\n",
    "```\n",
    "\n",
    "### Residuals\n",
    "- residuals $\\hat{W}_t = \\frac{X_t-\\hat{X}_t}{\\sqrt{r_{t-1}}}$\n",
    "- rescaled residuals $\\hat{R}_t = \\frac{\\hat{W_t}}{\\hat{\\sigma}}$\n",
    "    - $\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{t=1}^n\\hat{W}_t^2}$\n",
    "\n",
    "### Check Residuals are White Noise\n",
    "- SARIMA models assume shocks are white noise\n",
    "- white noise is a time series generated from uncorrelated variables and it is used as a model to build other series\n",
    "- {$Z_t$} are white noise when $E(Z_t) = 0$, $Var(Z_t) = \\sigma_z^2$, and $Cov(Z_t,\\;Z_s) = 0$ if $t \\neq s$\n",
    "    - is stationary because $E(Z_t)$ and $\\gamma_z(t,\\;t+h)$ don't depend on $t$\n",
    "    - $Z_t$ is an ARMA(0,0) model\n",
    "\n",
    "#### Preliminary Plot of Residuals\n",
    "- graph should appear stationary and all values should be between -3 and 3\n",
    "    - this means values are within three standard deviations of the mean\n",
    "    \n",
    "```\n",
    "library(TSA)\n",
    "# residuals\n",
    "resid <- residuals(mod)\n",
    "# standardized residuals\n",
    "residSt <- rstandard(mod)\n",
    "# plot residuals\n",
    "ts.plot(residSt, xlab='Minutes', ylab='Standardized Residuals', main='Time Series Plot of Residuals (White Noise)')\n",
    "```\n",
    "\n",
    "#### ACF and PACF of Residuals\n",
    "- residuals should resemble ARMA(0,0)\n",
    "    - at least 95% of the lags should be in the confidence interval containing 0\n",
    "    \n",
    "```\n",
    "h <- round(sqrt(length(resid)), digits=0)\n",
    "# acf of residuals\n",
    "acf(resid, lag.max = 60, main = 'ACF of Residuals')\n",
    "# pacf of residuals\n",
    "pacf(resid, lag.max = 60, main = 'PACF of Residuals')\n",
    "# check that AR is 0 for residuals\n",
    "ar(resid, aic=TRUE, order.max=NULL, method=c(\"yule-walker\"))\n",
    "```\n",
    "\n",
    "#### Portmanteau Statistics\n",
    "- $H_0:$ residuals are uncorrelated vs $H_a:$ residuals are correlated\n",
    "    - if p value $> 0.05$, residuals are white noise\n",
    "- $h\\approx\\sqrt{n}$\n",
    "- Box-Pierce\n",
    "    - $Q_W=n\\sum_{j=1}^h[\\hat\\rho(j)]^2$\n",
    "    - $Q_W\\sim\\chi_{h-p-q}^2$\n",
    "    - tests for linear dependence\n",
    "- Ljung\n",
    "    - $\\tilde{Q}_W=n(n+2)\\sum_{j=1}^h\\frac{[\\hat\\rho(j)]^2}{n-j}$\n",
    "    - $\\tilde{Q}_W\\sim\\chi_{h-p-q}^2$\n",
    "    - tests for linear dependence\n",
    "- Mcleod Li\n",
    "    - $\\tilde{Q}_{WW}=n(n+2)\\sum_{j=1}^h\\frac{[\\hat\\rho_{\\hat{W}\\hat{W}}(j)]^2}{n-j}$\n",
    "    - $\\tilde{Q}_{WW}\\sim\\chi_{h}^2$\n",
    "    - $\\bar{W^2}=\\frac{1}{n}\\sum_{t=1}^n\\hat{W_t}^2$\n",
    "    - tests for nonlinear dependence\n",
    "    \n",
    "```\n",
    "# Box-Pierec Test\n",
    "Box.test(resid, lag = h, type = c(\"Box-Pierce\"), fitdf = p+q+P+Q)\n",
    "# Ljung Test\n",
    "Box.test(resid, lag = h, type = c(\"Ljung-Box\"), fitdf = p+q+P+Q)\n",
    "# Mcleod Li Test\n",
    "Box.test(resid^2, lag = h, type = c(\"Ljung-Box\"), fitdf = 0)\n",
    "```\n",
    "\n",
    "### Check Residuals are Normally Distributed   \n",
    "- Gaussian white noise occurs when $Z_t \\overset{iid}{\\sim} N(0,\\;\\sigma_z^2$)\n",
    "- Maximum Likelihood estimators maximize the Gaussian Likelihood function, so, estimations are best when residuals come from a normal distribution\n",
    "    - Gaussian Likelihood function is used even if the process in non-Gaussian because of the Central Limit Theorem\n",
    "    \n",
    "#### Histogram of Residuals\n",
    "- residuals should have a bell curve shape, indicating normailty\n",
    "\n",
    "```\n",
    "# histogram of residuals\n",
    "hist(residSt, xlab='Standardized Residuals', main='Histogram of Residuals')\n",
    "```\n",
    "\n",
    "#### Normal Probability Plot\n",
    "- there should be a linear relationship between theoretical quantiles and sample quantiles\n",
    "\n",
    "```\n",
    "# Normal Probability Plot\n",
    "qqnorm(residSt)\n",
    "qqline(residSt)\n",
    "```\n",
    "\n",
    "#### Shapiro-Wilk Test\n",
    "- $H_0:$ residuals are from a normal distribution vs $H_a:$ residuals are not from a normal distribution\n",
    "\n",
    "```\n",
    "# Shapiro-Wilk Test\n",
    "shapiro.test(resid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting\n",
    "- best forecast minimizes the mean squared prediction error $E_n[(X_{n+h}-P_nX_{n+h})^2]$\n",
    "    - $X_{n+h}$ is the observed value\n",
    "    - $P_nX_{n+h}$ is the observed value\n",
    "- conditional expectation: $E_n(\\cdot) = E(\\cdot | X_1,...,\\;X_n)$\n",
    "\n",
    "### One Step Ahead Predictor\n",
    "- $P_nX_{n+1}=\\hat{X}_{n+1}$ is given by the innovation algorithm $\\hat{X}_{n+1} = \\sum_{j=1}^n\\theta_{nj}(X_{n+1-j}-\\hat{X}_{n+1-j})$ where innovation algorithm calculates $\\theta_{nj}$'s and gives prediction error $\\nu_n = E[(X_{n+1}-\\hat{X}_{n+1})^2]$\n",
    "\n",
    "### Forecasting AR(1)\n",
    "- h step ahead predictor: $P_nX_{n+h} = \\phi_1^hX_n$\n",
    "    - $P_nX_{n+1} = E_n(X_{n+1}) = E_n(\\phi_1X_n+Z_{n+1}) = \\phi_1P_nX_{n+1}=\\phi_1^2X_n$\n",
    "    - $P_nX_{n+2} = E_n(X_{n+2}) = \\phi_1E_n(1X_{n+1})+E(Z_{n+2}) = \\phi_1X_n$\n",
    "- the best forecast converges to the process mean for stationary AR(1) processes and large lead time h\n",
    "\n",
    "Prediction Error\n",
    "- $e_n(h) = \\phi_1^{h-1}Z_{n+1} + \\phi_2^{h-2}Z_{n+2}+...+\\phi_1Z_{n+h-1}+Z_{n+h}$\n",
    "    - $e_n(1) = X_{n+1} - P_nX_{n+1} = (\\phi_1X_n+Z_{n+1}) - (\\phi_1X_n) = Z_{n+1}$\n",
    "    - $e_n(2) = X_{n+2} - P_nX_{n+2} = (\\phi_1X_{n+1}+Z_{n+2}) - (\\phi_1^2X_n) = \\phi_1(X_{n+1}-\\phi_1X_n) + Z_{n+2} = \\phi(X_{n+1} - P_{n+1}) + Z_{n+2} = \\phi_1e_n(1) +Z_{n+2} = \\phi_1Z_{n+1} + Z_{n+2}$\n",
    "\n",
    "Variance\n",
    "- $var(e_n(h)) = \\sigma_z^2\\frac{1-\\phi_1^{2h}}{1-\\phi_1^2}$\n",
    "    - $var(e_n(h)) = var(\\phi_1^{h-1}Z_{n+1} + \\phi_2^{h-2}Z_{n+2}+...+\\phi_1Z_{n+h-1}+Z_{n+h}) = \\sigma_z^2(\\phi_1^{2(h-1)}+ ... + \\phi_1^2 + 1) = \\sigma_z^2\\frac{1-\\phi_1^{2h}}{1-\\phi_1^2}$\n",
    "\n",
    "### Forecasting AR(p)\n",
    "- recursive formula for $h>1$\n",
    "    - one step ahead: $P_nX_{n+1} = E_n(X_{n+1}) = \\phi_1E_n(X_n) + ... + \\phi_pE_n(X_{n+1-p}) + E_nZ_{n+1} = \\phi_1X_n + ... + \\phi_pX_{n+1-p}$\n",
    "    - two steps ahead: $P_nX_{n+2} = E_n(X_{n+2}) = \\phi_1E_n(X_{n+1}) + ... + \\phi_pE_n(X_{n+2-p}) + E_nZ_{n+2} = \\phi_1P_nX_{n+1} + ... + \\phi_pX_{n+2-p}$\n",
    "- with intercept or mean\n",
    "    - in R, arima() gives the mean of the process, but it is labeled as intercept\n",
    "    - with intercept: $X_t = \\phi_1X_{t-1} + ... + \\phi_pX_{t-p} + \\alpha + Z_t$ \n",
    "        - $P_nX_{n+1} = E_n(X_{n+1}) = \\phi_1E_n(X_n)+...+\\phi_pE_n(X_{n+1-p})+E_n(\\alpha)+E_n(Z_{n+1}) = \\phi_1X_n+...+\\phi_pX_{n+1-p} + \\alpha$\n",
    "    - with mean: $X_t - \\mu = \\phi_1(X_{t-1}-\\mu) + ... + \\phi_p(X_{t-p}-\\mu) + Z_t$ \n",
    "        - let $Y_t = X_t - \\mu$\n",
    "        - $P_nY_{n+1} = E_n(Y_{n+1}) = \\phi_1E_n(Y_n)+...+\\phi_pE_n(Y_{n+1-p})+E_n(Z_{n+1}) = \\phi_1Y_n+...+\\phi_pY_{n+1-p}$\n",
    "        - then $P_nX_{n+1} = \\phi_1Y_n+...+\\phi_pY_{n+1-p} + \\mu$\n",
    "    - conversion between intercept and mean\n",
    "        - $\\mu = E(X_t) = \\alpha + \\phi_1\\mu + ... + \\phi_p\\mu$\n",
    "        - $\\alpha = \\mu(1-\\phi_1-...-\\phi_p)$\n",
    "        - if $\\phi_j \\neq 0$ for all $j$, $\\alpha \\neq \\mu$\n",
    "\n",
    "### Forecasting MA(q)\n",
    "- one step ahead: $P_nX_{n+1}=\\hat{X}_{n+1} = \\theta_{n1}(X_n-\\hat{X}_n) + ... + \\theta_{nq}(X_{n+1-q}-\\hat{X}_{n+1-q}) = \\sum_{j=1}^q\\theta_{nj}(X_{n+1-j}-\\hat{X}_{n+1-j})$\n",
    "- h steps ahead: $P_nX_{n+h} = \\sum_{j=h}^q\\theta_{n+h-1,j}(X_{n+h-j}-\\hat{X}_{n+h-j})$\n",
    "    - use the innovation algorithm to find coefficients\n",
    "\n",
    "Variance\n",
    "- $\\nu_n = E[(X_{n+1}-\\hat{X}_{n+1})^2] = \\hat{\\sigma}_z^2r_n$\n",
    "    - given by innovation algorithm\n",
    "\n",
    "Calculation <br>\n",
    "\n",
    "1: use Tower property of conditional expectation $P_nX_{n+h} = P_n(P_{n+h-1}X_{n+h})$\n",
    "- by definition, $P_nX_{n+h} = E_n(X_{n+h})$\n",
    "- by Tower Property, $E_n(X_{n+h}) = E_n[E_n(X_{n+h}|X_1,...,X_{n+h-1})]$\n",
    "- $E_n(X_{n+h}|X_1,...,X_{n+h-1}) = P_{n+h-1}X_{n+h}$ is the one-step ahead predictor of $X_{n+h}$ <br>\n",
    "\n",
    "2: use innovations for one step ahead predictor $P_{n+h-1}X_{n+h} = \\sum_{j=1}^q\\theta_{n+h-1,j}(X_{n+h-j}-\\hat{X}_{n+h-j})$ with coefficients from innovation algorithm <br>\n",
    "\n",
    "3: combine formulas using linearity of conditional expectation $P_nX_{n+h} = P_n(\\sum_{j=1}^q\\theta_{n+h-1,j}(X_{n+h-j}-\\hat{X}_{n+h-j})) = \\sum_{j=1}^q\\theta_{n+h-1,j}P_n(X_{n+h-j}-\\hat{X}_{n+h-j})$ <br>\n",
    "\n",
    "4: adjust bounds of summation\n",
    "- if $j<h$, $P_n(X_{n+h-j}-\\hat{X}_{n+h-j}) = 0$ because $P_n\\hat{X}_{n+h-j} = E_n[E_n(X_{n+h=j}|X_{n+h-j}, ..., X_1)] = E_nX_{n+h-j} = P_nX_{n+h-j}$ \n",
    "- if $j \\geq h$,$P_n(X_{n+h-j}\\hat{X}_{n+h-j}) = X_{n+h-j} - \\hat{X}_{n+h-j}$ because $P_nX_{n+h-j} = E_n(X_{n+h-j}) = X_{n+h-j}$ and $P_n\\hat{X}_{n+h-j} = E_n[E(X_{n+h-j}|X_{n+h-j}, ..., X_1)] = E(X_{n+h-j}|X_{n+h-j-1}, ..., X_1)=\\hat{X}_{n+h-j}$\n",
    " \n",
    "### Forecasting ARMA(p,q)\n",
    "- let $m = max${$p,q$} and $n>m$\n",
    "- $P_nX_{n+h} = \\sum_{i=1}^p\\phi_iP_nX_{n+h-i} + \\sum_{j=h}^q\\theta_{n+h-1,j}(X_{n+h-j} - \\hat{X}_{n+h-j})$\n",
    "    - $P_nX_{n+h} = P_n(P_{n+h-1}X_{n+h}) = P_n[\\sum_{i=1}^p\\phi_iP_nX_{n+h-i} + \\sum_{j=1}^q\\theta_{n+h-1,j}(X_{n+h-j} - \\hat{X}_{n+h-j})] = \\sum_{i=1}^p\\phi_iP_nX_{n+h-i} + \\sum_{j=h}^q\\theta_{n+h-1,j}(X_{n+h-j} - \\hat{X}_{n+h-j})$\n",
    "    \n",
    "Confidence Interval\n",
    "- if $X_t$ is Gaussian, h step predictors and predictor errors are Gaussian\n",
    "- 95% confidence interval for $X_{n+h}$ is $P_nX_{n+h} \\pm 1.96\\sigma_n(h)$\n",
    "    - $\\sigma_n^2(h) \\approx \\sigma_z^2\\sum_{j=1}^{h-1}\\psi_j^2$ where $\\psi(z) = \\frac{\\theta(z)}{\\phi(z)}$\n",
    "\n",
    "### Predict Next Five Observations\n",
    "\n",
    "```\n",
    "# predictions for sarima with no trend\n",
    "pred <- predict(mod, n.ahead=5)\n",
    "# predictions for sarima with trend\n",
    "pred <- predict(mod, n.ahead=5, newxreg=(length(modelData)-d+1, length(modelData-d+5)\n",
    "# confidence interval\n",
    "lower <- pred(dollar)pred - 1.96*pred(dollar)se\n",
    "upper <- pred(dollar)pred + 1.96*pred(dollar)se\n",
    "```\n",
    "\n",
    "### Plot Predictions\n",
    "\n",
    "#### Untransformed Data\n",
    "\n",
    "```\n",
    "# plot time series\n",
    "ts.plot(data, xlab='time units', ylab='y units', main='Forecasted Values')\n",
    "# plot predicted points\n",
    "points(length(modelData)+1:length(modelData)+5, pred(dollar)pred, col = 'red')\n",
    "# draw confidence interval\n",
    "lines(length(modelData)+1:length(modelData)+5, lower, lty=2, col = 'green')\n",
    "lines(length(modelData)+1:length(modelData)+5, upper, lty=2, col = 'green')\n",
    "# add a legend\n",
    "legend('topleft', c('observed values', 'forecasted values','forecasted confidence interval'), fill=c('black', \n",
    "        'red','green'))\n",
    "```\n",
    "\n",
    "#### Transformed Data\n",
    "- if series was transformed to stabilize variance, need to transform forecasts back to original units\n",
    "Naive Forecast\n",
    "- $V_t$ is the original data and $Y_t=ln(V_t)$ is transformed data\n",
    "- naive forecast of $V_{n+h}$ is $\\hat{V}_n(h)=e^{\\hat{Y}_n(h)}$\n",
    "    - forecast optimizes the median instead of the mean\n",
    "    - the data is non-Gaussian so the mean is skewed\n",
    "    - therefore, the mean changes in the transformation, but the median is preserved\n",
    "\n",
    "```\n",
    "# plot transformed time series\n",
    "ts.plot(dataBC, xlab='time units', ylab='y transformed units', main='Forecasted Values: Transformed Data')\n",
    "# plot predicted points\n",
    "points(length(modelData)+1:length(modelData)+5, pred(dollar)pred, col = 'red')\n",
    "# draw confidence interval\n",
    "lines(length(modelData)+1:length(modelData)+5, lower, lty=2, col = 'green')\n",
    "lines(length(modelData)+1:length(modelData)+5, upper, lty=2, col = 'green')\n",
    "# add a legend\n",
    "legend('topleft', c('observed values', 'forecasted values','forecasted confidence interval'), \n",
    "        fill=c('black','red','green'))\n",
    "```\n",
    "\n",
    "Original units\n",
    "\n",
    "```\n",
    "# convert log transformed data back to original units\n",
    "predOrig <- exp(1)^pred(dollar)pred\n",
    "lowerOrig <- exp(1)^lower\n",
    "upperOrig <- exp(1)^upper\n",
    "# plot data with original units\n",
    "ts.plot(data, xlab='time units', ylab='y units', main='Forecasted Values: Original Data')\n",
    "points(length(modelData)+1:length(modelData)+5, predOrig, col = 'red')\n",
    "lines(length(modelData)+1:length(modelData)+5, lowerOrig, lty=2, col = 'green')\n",
    "lines(length(modelData)+1:length(modelData)+5, upperOrig, lty=2, col = 'green')\n",
    "# add a legend\n",
    "legend('topleft', c('observed values', 'forecasted values', 'forecasted confidence interval'), \n",
    "        fill=c('black','red','green'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
